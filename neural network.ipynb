{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.signal import savgol_filter as sgf\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"classification\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# featurize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Synth = pd.read_csv('CsPbBr_synthesis.csv', index_col = 0)\n",
    "\n",
    "def conc_factor(di=1,mw=1,density=1,conc=0):\n",
    "    result = 0\n",
    "    if conc != 0:\n",
    "        return conc/500/di\n",
    "    else:\n",
    "        volume = 1 #in ul\n",
    "        mass = (volume/1000000)/di*density #g\n",
    "        amount = mass/mw #mol\n",
    "        result = amount*1000*1000000/500\n",
    "        return result #converting into mM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dilution ratio:\n",
    "#Cs Pb 200,OLA 100, Br OA 80\n",
    "SynData_new = pd.DataFrame()\n",
    "#Cesium Concentration in mM\n",
    "SynData_new['Cs'] = Synth['Cs']*conc_factor(di=200,conc=1000)\n",
    "#Lead Concentration in mM\n",
    "SynData_new['Pb'] = Synth['Pb']*conc_factor(di=200,conc=667)\n",
    "#Oleylamine Concentration in mM\n",
    "SynData_new['OLA'] = Synth['OLA']*conc_factor(di=100,mw=267.5,density=813)\n",
    "#Oleic Acid Concentration in mM\n",
    "SynData_new['OA'] = (Synth['OA']+Synth['Cs']+Synth['Pb'])*conc_factor(di=80,mw=282.5,density=895)\n",
    "#Benzoyl Bromide Concentration in mM\n",
    "SynData_new['Br'] = Synth['Br']*conc_factor(di=80,mw=185,density=1570)\n",
    "#Temperature in Celcius\n",
    "SynData_new['Temp'] = Synth['Temp'] + 273\n",
    "#Normalized Concentration of Products\n",
    "SynData_new['PbBr3-'] = Synth['PbBr3-']\n",
    "SynData_new['Cs4PbBr6'] = Synth['Cs4PbBr6']\n",
    "SynData_new['CsPb2Br5'] = Synth['CsPb2Br5']\n",
    "SynData_new['PbBr2'] = Synth['PbBr2']\n",
    "SynData_new['1ML'] = Synth['1 ML']\n",
    "SynData_new['2ML'] = Synth['2 ML']\n",
    "SynData_new['3ML'] = Synth['3 ML']\n",
    "SynData_new['4ML'] = Synth['4 ML']\n",
    "SynData_new['CsPbBr3'] = Synth['CsPbBr3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = ['Cs4PbBr6', 'CsPb2Br5', 'CsPbBr3', 'PbBr3-', 'PbBr2', '1ML', '2ML', '3ML', '4ML']\n",
    "id = 8\n",
    "\n",
    "X = np.array(SynData_new[['Cs', 'Pb', 'OLA', 'OA', 'Br', 'Temp']])\n",
    "y = np.array(SynData_new[product[id]])\n",
    "y_logged = np.log(1 + np.log( 1 + np.log( 1 + np.log(1 + y))))\n",
    "\n",
    "\n",
    "# 'Cs4PbBr6', 'CsPb2Br5', 'CsPbBr3', 'PbBr3-', 'PbBr2', '1ML', '2ML', '3ML', '4ML'\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for id,name in enumerate(product):\n",
    "#     y = np.array(SynData_new[product[id]])\n",
    "#     y_logged = np.log( 1 + np.log(1 + y))\n",
    "#     bin = 30\n",
    "#     fig, ax = plt.subplots(1,2, sharey=True, figsize=(16,5))\n",
    "#     ax[0].hist(y, bins=bin)\n",
    "#     ax[1].hist(y_logged, bins=bin)\n",
    "#     ax[0].set_title('Original')\n",
    "#     ax[1].set_title('Logged')\n",
    "#     fig.suptitle(f'{product[id]}')\n",
    "#     fig.savefig(f'images/hist_{product[id]}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split train and test set:\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize hyper parameters for MLP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize hyper parameter:\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import fmin, tpe, space_eval, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def hyperoptoutput2param(best):\n",
    "    \n",
    "    '''Change hyperopt output to dictionary with values '''\n",
    "    \n",
    "    for key in best.keys():\n",
    "        if key in hyper_dict.keys():\n",
    "            best[key] = hyper_dict[key][ best[key] ] \n",
    "            \n",
    "    return best\n",
    "\n",
    "# Define a dicionary for each parameter range \n",
    "\n",
    "hyper_dict = {\n",
    "    \"depth\": [1, 2, 3],\n",
    "    \"width\": [32, 64, 128, 192],\n",
    "    \"optimizer\": ['adam'],\n",
    "    \"activation\": ['relu', 'logistic', 'tanh'],\n",
    "    \"alpha\":[0.02, 0.04, 0.08, 0.16]\n",
    "}\n",
    "\n",
    "parameter_space =  { \"depth\": hp.choice(\"depth\", hyper_dict['depth']),\n",
    "                    \"width\": hp.choice(\"width\", hyper_dict['width']),\n",
    "                    \"optimizer\": hp.choice(\"optimizer\", hyper_dict['optimizer']), \n",
    "                    \"activation\": hp.choice(\"activation\", hyper_dict['activation']), \n",
    "                    \"alpha\": hp.choice(\"alpha\", hyper_dict['alpha'])\n",
    "                    }\n",
    "\n",
    "\n",
    "# Evaludation function \n",
    "\n",
    "def model_eval(args):\n",
    "\n",
    "    '''Take suggested arguments and perform model evaluation'''\n",
    "\n",
    "    # generate tuple input for hidden_layer_sizes \n",
    "    hidden_layers = tuple( [args['width']] * args['depth'] )\n",
    "\n",
    "    # your code here to train MLPRegressors and trun CV score on the training data \n",
    "    activation = args['activation']\n",
    "    solver = args['optimizer']\n",
    "    alpha = args['alpha']\n",
    "    \n",
    "    mlp = MLPRegressor(hidden_layer_sizes=hidden_layers, activation=activation, solver=solver, alpha=alpha, early_stopping=False, max_iter=10000, random_state=2) # Increase \"max_iter\" to ensure convergence\n",
    "    cv_score = -cross_val_score(mlp, X_train, y_train, cv=5, scoring='r2').mean()\n",
    "    return cv_score\n",
    "\n",
    "\n",
    "print(\"Start trials\") \n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(model_eval, parameter_space, algo=tpe.suggest, max_evals=40, trials=trials) # this will take a while to run \n",
    "best = hyperoptoutput2param(best)\n",
    "\n",
    "print(\"best parameter set: {}\".format(best))\n",
    "print(\"best cv {}\".format(trials.best_trial['result']['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a MLP\n",
    "regr_new = MLPRegressor(hidden_layer_sizes= (192, 192, 192), activation='relu', solver='adam', alpha=0.08, max_iter=10000, early_stopping=False, random_state=2)\n",
    "regr_new.fit(X_train, y_train)\n",
    "train_prediction = regr_new.predict(X_train)\n",
    "train_truth = y_train\n",
    "test_prediction = regr_new.predict(X_test)\n",
    "test_truth = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Datasets\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Generate dataset \n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(X)  # store X as a pytorch Tensor\n",
    "        self.y = torch.Tensor(y)  # store y as a pytorch Tensor\n",
    "        self.len=len(self.X)                # number of samples in the data \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # your implementation here: \n",
    "        return self.X[index], self.y[index] # implement your __getitem__function here \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_logged, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "#Build dataset \n",
    "traindata = SequenceDataset(X=X_train, y=y_train)\n",
    "valdata = SequenceDataset(X=X_val, y=y_val)\n",
    "testdata = SequenceDataset(X=X_test, y=y_test)\n",
    "\n",
    "# Build dataloader \n",
    "batchsize = 128\n",
    "train_loader = DataLoader(dataset=traindata,batch_size=batchsize,shuffle=True)\n",
    "val_loader = DataLoader(dataset=valdata,batch_size=batchsize,shuffle=True)\n",
    "test_loader = DataLoader(dataset=testdata,batch_size=batchsize,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LMPseq(torch.nn.Module) :\n",
    "\n",
    "    def __init__(self, input_dim, activation = 'relu', hidden_dim = (128, 128, 128)) :\n",
    "        super().__init__()\n",
    "        \n",
    "        # define a mlp regressor \n",
    "        if activation == 'relu':\n",
    "\n",
    "            activation = nn.ReLU()\n",
    "\n",
    "        if activation == 'tanh':\n",
    "\n",
    "            activation = nn.Tanh()\n",
    "\n",
    "        if activation == 'logistic':\n",
    "\n",
    "            activation = nn.LogSigmoid()\n",
    "\n",
    "        # print(activation)\n",
    "\n",
    "        mlp = nn.Sequential(nn.Linear(input_dim, hidden_dim[0]), \n",
    "                            activation)\n",
    "\n",
    "        for idx, dim in enumerate(hidden_dim[1:]):\n",
    "\n",
    "            mlp.append(nn.Linear(hidden_dim[idx - 1], dim))\n",
    "            mlp.append(activation)\n",
    "\n",
    "        self.mlp = mlp\n",
    "\n",
    "        outlayer = nn.Sequential(nn.Linear(dim, 1),\n",
    "                                 nn.Sigmoid())        \n",
    "        \n",
    "        self.outlayer = outlayer\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Apply lstm \n",
    "        # lstm_out, (ht, ct) = self.lstm(x)\n",
    "        # pass ouput into a MLP \n",
    "        output = self.mlp(x)\n",
    "        # transform output into probabilites \n",
    "        output = self.outlayer(output)\n",
    "        # return probabilities \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, dataloader, optimizer):\n",
    "    \n",
    "    '''\n",
    "    A function train on the entire dataset for one epoch .\n",
    "    \n",
    "    Args: \n",
    "        model (torch.nn.Module): your model from before \n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader object for the train data\n",
    "        optimizer (torch.optim.Optimizer(()): optimizer object to interface gradient calculation and optimization \n",
    "        device (str): Your device (usually 'cuda:0' for your GPU)\n",
    "        \n",
    "    Returns: \n",
    "        float: loss averaged over all the batches \n",
    "    \n",
    "    '''\n",
    "\n",
    "    epoch_loss = []\n",
    "    model.train() # Set model to training mode \n",
    "    \n",
    "    for batch in dataloader:    \n",
    "        X, y = batch\n",
    "        X = X\n",
    "        y = y\n",
    "        \n",
    "        # train your model on each batch here \n",
    "        y_pred = model(X)\n",
    "        \n",
    "        loss = F.binary_cross_entropy(y_pred.squeeze(), y.squeeze())# fill in loss here\n",
    "        # loss = F.mse_loss(y_pred.squeeze(), y.squeeze())# fill in loss here\n",
    "        epoch_loss.append(loss.item())\n",
    "        # run backpropagation given the loss you defined\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.array(epoch_loss).mean()\n",
    "\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    \n",
    "    '''\n",
    "    A function validate on the validation dataset for one epoch .\n",
    "    \n",
    "    Args: \n",
    "        model (torch.nn.Module): your model for before \n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader object for the validation data\n",
    "        device (str): Your device (usually 'cuda:0' for your GPU)\n",
    "        \n",
    "    Returns: \n",
    "        float: loss averaged over all the batches \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    val_loss = []\n",
    "    model.eval() # Set model to evaluation mode \n",
    "    with torch.no_grad():    \n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X\n",
    "            y = y\n",
    "            \n",
    "            # validate your model on each batch here \n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = F.binary_cross_entropy(y_pred.squeeze(), y.squeeze())# fill in loss here\n",
    "            # loss = F.mse_loss(y_pred.squeeze(), y.squeeze())# fill in loss here\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    return np.array(val_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LMPseq(6, activation='relu', hidden_dim = (128,128,128))\n",
    "\n",
    "optimizer = torch.optim.Adam(list(model.parameters()), lr=0.002)\n",
    "\n",
    "val_loss_curve = []\n",
    "train_loss_curve = []\n",
    "difference = 1\n",
    "\n",
    "for epoch in range(30):\n",
    "    \n",
    "    # Compute train your model on training data\n",
    "    epoch_loss = train(model, train_loader, optimizer)\n",
    "    \n",
    "    # Validate your on validation data \n",
    "    val_loss = validate(model, val_loader) \n",
    "    \n",
    "    # Record train and loss performance \n",
    "    train_loss_curve.append(epoch_loss)\n",
    "    val_loss_curve.append(val_loss)\n",
    "    \n",
    "    # print(epoch, epoch_loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(val_loss_curve, label=\"validation\")\n",
    "plt.plot(train_loss_curve, label=\"train\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to compute AUC on test data \n",
    "pred_train = model(traindata.X)\n",
    "pred_train = pred_train.detach().numpy()[:,0]\n",
    "test_score = sklearn.metrics.r2_score(y_train, pred_train.squeeze())\n",
    "print(\"R2 on the train dataset is {}\".format(test_score) ) \n",
    "\n",
    "pred_test = model(testdata.X)\n",
    "pred_test = pred_test.detach().numpy()[:,0]\n",
    "test_score = sklearn.metrics.r2_score(y_test, pred_test.squeeze())\n",
    "print(\"R2 on the test dataset is {}\".format(test_score) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, train_loader, optimizer, max_iteration = 400, lr = 0.002, tol=0.0001):\n",
    "    \n",
    "    convergence = True\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=lr)\n",
    "    val_loss_curve = []\n",
    "    train_loss_curve = []\n",
    "    difference = 1\n",
    "\n",
    "    for epoch in range(max_iteration):\n",
    "        \n",
    "        if difference > tol:\n",
    "    \n",
    "            # Compute train your model on training data\n",
    "            epoch_loss = train(model, train_loader, optimizer)\n",
    "            \n",
    "            # Validate your on validation data \n",
    "            val_loss = validate(model, val_loader) \n",
    "\n",
    "            difference = np.abs(val_loss - epoch_loss)\n",
    "            \n",
    "            # Record train and loss performance \n",
    "            train_loss_curve.append(epoch_loss)\n",
    "            val_loss_curve.append(val_loss)\n",
    "        \n",
    "    if difference > tol:\n",
    "\n",
    "        print('fail to converge')\n",
    "        convergence = False\n",
    "\n",
    "    return model, convergence\n",
    "\n",
    "model = LMPseq(6, activation='relu', hidden_dim = (128,128,128))\n",
    "trained_model, convergence = evaluate(model, train_loader, optimizer, max_iteration = 400, lr = 0.002, tol=0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate different trails:\n",
    "\n",
    "num_trail = 100\n",
    "\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "for i in range(num_trail):\n",
    "\n",
    "    model = LMPseq(6, activation='relu', hidden_dim = (128,128,128))\n",
    "    trained_model, convergence = evaluate(model, train_loader, optimizer, max_iteration = 400, lr = 0.002, tol=0.0001)\n",
    "\n",
    "    if convergence:\n",
    "\n",
    "        pred_train = trained_model(traindata.X)\n",
    "        pred_train = pred_train.detach().numpy()[:,0]\n",
    "        train_score = sklearn.metrics.r2_score(y_train, pred_train.squeeze())\n",
    "        # print(\"R2 on the train dataset is {}\".format(train_score) ) \n",
    "\n",
    "        pred_test = trained_model(testdata.X)\n",
    "        pred_test = pred_test.detach().numpy()[:,0]\n",
    "        test_score = sklearn.metrics.r2_score(y_test, pred_test.squeeze())\n",
    "        # print(\"R2 on the test dataset is {}\".format(test_score))\n",
    "\n",
    "        train_score_list.append(train_score)\n",
    "        test_score_list.append(test_score)\n",
    "\n",
    "\n",
    "train_score_list = np.array(train_score_list)\n",
    "test_score_list = np.array(test_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_score_list.mean())\n",
    "print(train_score_list.std())\n",
    "print(test_score_list.mean())\n",
    "print(test_score_list.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot R2:\n",
    "\n",
    "pred_train = trained_model(traindata.X)\n",
    "pred_train = pred_train.detach().numpy()[:,0]\n",
    "train_score = sklearn.metrics.r2_score(y_train, pred_train.squeeze())\n",
    "print(\"R2 on the train dataset is {}\".format(train_score) ) \n",
    "\n",
    "pred_test = trained_model(testdata.X)\n",
    "pred_test = pred_test.detach().numpy()[:,0]\n",
    "test_score = sklearn.metrics.r2_score(y_test, pred_test.squeeze())\n",
    "print(\"R2 on the test dataset is {}\".format(test_score))\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5))\n",
    "ax.scatter(pred_train.squeeze(), y_train, label='train', alpha=0.5)\n",
    "ax.scatter(pred_test.squeeze(), y_test, label='test', alpha=0.5, c='orange')\n",
    "\n",
    "ax.set_ylabel(f\"True yield\")\n",
    "ax.set_xlabel(f\"Predicted yield\")\n",
    "ax.set_title(f'{product[id]}')\n",
    "\n",
    "ax.text(0.5,0, 'R2_test:{:.2f}'.format(test_score))\n",
    "ax.text(0.55,0.05, 'R2_train:{:.2f}'.format(train_score))\n",
    "\n",
    "# fig.savefig(f'result_{product[id]}.png')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1c8b4f8d463ec2965e2635f05ad9fc24d420c69436f1b0cec9499531a516565"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('xi_python3.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
